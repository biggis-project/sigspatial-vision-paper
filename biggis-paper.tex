\documentclass{sig-alternate-05-2015}

\usepackage{xcolor}
\usepackage{pifont}
\usepackage{paralist} % inparaenum support

\usepackage[utf8]{inputenc} 
\usepackage[T1]{fontenc}
%\usepackage{microtype}

\newcommand{\quadrat}{\ding{110}}%


\begin{document}
\sloppy
% No indent of paragraph
\parindent 0pt

% Copyright
\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


% DOI
%\doi{10.475/123_4}

% ISBN
%\isbn{123-4567-24-567/08/06}

%Conference
\conferenceinfo{ACM SigSpatial '16}{Oct 31 - Nov 3, 2016, San Francisco Bay
Area, California, USA}

%\acmPrice{\$15.00}

%
% --- Author Metadata here ---
%\conferenceinfo{WOODSTOCK}{'97 El Paso, Texas USA}
%\CopyrightYear{2007} % Allows default copyright year (20XX) to be over-ridden
%- IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data
%(0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---
\title{BigGIS: A Continuous Refinement Approach to Master Heterogeneity and
Uncertainty in Spatio-Temporal Big Data (Vision Paper)}
%\titlenote{(Produces the permission block, and
%copyright information). For use with
%SIG-ALTERNATE.CLS. Supported by ACM.}}
%\subtitle{[Extended Abstract]
%\titlenote{A full version of this paper is available as
%\textit{Author's Guide to Preparing ACM SIG Proceedings Using
%\LaTeX$2_\epsilon$\ and BibTeX} at
%\texttt{www.acm.org/eaddress.htm}}}
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{1} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
\alignauthor
Patrick Wiener\textsuperscript{1}, Manuel Stein\textsuperscript{2}, Daniel
Seebacher\textsuperscript{2}, Julian Bruns\textsuperscript{3}, Matthias
Frank\textsuperscript{3}, Viliam Simko\textsuperscript{3},
Stefan Zander\textsuperscript{3}, Jens
Nimis\textsuperscript{1}\\~\\
\affaddr{\textsuperscript{1}University of Applied Sciences Karlsruhe,
Karlsruhe, Germany}\\
\email{\{patrick.wiener, jens.nimis\}@hs-karlsruhe.de}\\
\affaddr{\textsuperscript{2}Data Analysis and Visualization Group, University
of Konstanz, Konstanz, Germany}\\
\email{\{stein, seebacher\}@dbvis.inf.uni-konstanz.de}\\
\affaddr{\textsuperscript{3}FZI Research Center for Information Technology,
Karlsruhe, Germany}\\
\email{\{bruns, frank, simko, zander\}@fzi.de}
}
%\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
%\titlenote{Maybe put address, e-mail here if allowed}
%\alignauthor
%%Patrick Wiener,\\Prof. Dr. Jens Nimis\\
%Patrick Wiener\\
%       \affaddr{University of Applied Sciences Karlsruhe}\\
%%       \affaddr{Moltkestr. 30}\\
%       \affaddr{Karlsruhe, Germany}\\
%       \email{firstname.lastname@hs-karlsruhe.de}
%% 2nd. author
%\alignauthor
%%Julian Bruns,\\Matthias Frank,\\Dr. Viliam Simko\\
%Julian Bruns\\
%       \affaddr{FZI Research Center for Information Technology}\\
%%       \affaddr{Haid-und-Neu-Str. 10-14}\\
%       \affaddr{Karlsruhe, Germany}\\
%       \email{lastname@fzi.de}
%% 3rd. author
%\alignauthor
%%Manuel Stein,\\Daniel Seebacher\\
%Daniel Seebacher\\
%       \affaddr{University of Konstanz}\\
%       \affaddr{Konstanz, Germany}\\
%       \email{firstname.lastname@uni-konstanz.de}
%\and  % use '\and' if you need 'another row' of author names
%%% 4th. author
%\alignauthor
%Matthias Frank\\
%       \affaddr{FZI Research Center for Information Technology}\\
%%       \affaddr{Haid-und-Neu-Str. 10-14}\\
%       \affaddr{Karlsruhe, Germany}\\
%       \email{lastname@fzi.de}
%%% 5th. author
%\alignauthor
%Manuel Stein\\
%%       \affaddr{Data Analysis and Visualization Group}\\
%       \affaddr{University of Konstanz}\\
%       \affaddr{Konstanz, Germany}\\
%       \email{firstname.lastname@uni-konstanz.de}
%%% 6th. author
%\alignauthor
%Prof. Dr.-Ing. Jens Nimis\\
%       \affaddr{University of Applied Sciences Karlsruhe}\\
%%       \affaddr{Moltkestr. 30}\\
%       \affaddr{Karlsruhe, Germany}\\
%       \email{firstname.lastname@hs-karlsruhe.de}
%}
% There's nothing stopping you putting the seventh, eighth, etc.
% author on the opening page (as the 'third row') but we ask,
% for aesthetic reasons that you place these 'additional authors'
% in the \additional authors block, viz.
%\additionalauthors{Dr. Viliam Simko (FZI Research Center for Information
%Technology, email: {\texttt{lastname@fzi.de}})}
%\date{30 July 1999}
% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\maketitle
\begin{abstract}
Geographic information systems (GIS) are important for decision support
based on spatial data. Due to technical and economical progress an ever
increasing number of data sources are available leading to a rapidly growing
fast and unreliable amount of data that can be beneficial
\begin{inparaenum}[(1)]
\item in the approximation of multivariate and causal predictions of future
values as well as
\item in robust and proactive decision-making processes. 
\end{inparaenum}
However, today's GIS are not designed for such big data demands and require new
methodologies to effectively model uncertainty and generate meaningful
knowledge. As a consequence, we introduce \textit{BigGIS}, a predictive and
prescriptive spatio-temporal analytics platform, that symbiotically
combines big data analytics, semantic web technologies and visual analytics
methodologies. We present a novel continuous refinement model and show future
challenges as an intermediate result of a collaborative research project into
big data methodologies for spatio-temporal analysis and design for a big data
enabled GIS.
\end{abstract}


%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below. 
%
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10002951.10003227.10003236.10003237</concept_id>
<concept_desc>Information systems~Geographic information systems</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10002951.10003227.10010926</concept_id>
<concept_desc>Information systems~Computing platforms</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10003120.10003145.10003147.10010365</concept_id>
<concept_desc>Human-centered computing~Visual analytics</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10011007.10011006.10011039.10011311</concept_id>
<concept_desc>Software and its engineering~Semantics</concept_desc>
<concept_significance>100</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Information systems~Geographic information systems}
\ccsdesc[500]{Information systems~Computing platforms}
\ccsdesc[500]{Human-centered computing~Visual analytics}
\ccsdesc[500]{Software and its engineering~Semantics}


%
% End generated code
%

%
%  Use this command to print the description
%
\printccsdesc

% We no longer use \terms command
%\terms{Theory}
%\category{C.1.3}{Other Architecture Styles}{Data-flow architectures}
%\category{H.1.2}{User/Machine Systems}{Human information processing}
%\category{H.2.8}{Database Applications}{Data mining, Spatial databases and
%GIS}

\keywords{knowledge generation, big data analytics, data architecture}

\section{Introduction}
\label{sec:intro}
GIS have long been used to support the decision-making
process~\cite{Crossland1995} in many domains like civil planning, environment
and nature protection or emergency management. Thereby, geospatial data have
always been big data. Petabytes of remotely sensed archival geodata
(\textit{volume}) and a rapidly increasing amount of real-time sensor data
streams (\textit{velocity}) accelerate the need for big data analytics in order
to effectively model and efficiently process complex geo-temporal problems. In
the past, limited access to computing power has been a
bottleneck~\cite{OGC2013}. However, in the era of cloud computing,
leveraging cloud-based resources is a widely adopted pattern (hardware level).
In addition, with the advent of big data analytics, performing massively
parallel analytical tasks on large-scale data at rest or data in motion is as
well becoming a feasible approach shaping the design of today's GIS
(software-level). Although scaling out enables GIS to tackle the aforementioned
big data induced requirements, there are still two major open issues. Firstly,
dealing with varying data types across multiple data sources (\textit{variety})
lead to data and schema heterogeneity, e.g., to describe locations such as
addresses, relative spatial relationships or different coordinates reference
systems~\cite{Frank.2016b}. Secondly, modelling the inherent uncertainties in
data (\textit{veracity}), e.g., real-world noise and errorneous values due to
the nature of the data collecting process. Both being crucial tasks in data
management and analytics that directly affect the information retrieval and
decision-making quality and moreover the generated knowledge on human-side
(\textit{value}). Current approaches mainly address batch and stream analytics
in their design that is oftentimes implemented as a closed unified analytical
system~\cite{Thakur2015}. While the importance of such systems to
efficiently deal with large amount of data is obvious, computers miss the
cognition and perception of human analysis to create hidden connections between
data and problem domain~\cite{SSS+14a}.

In this paper, we present the vision of \textit{BigGIS}, a next generation 
predictive and prescriptive GIS, that leverages big data analytics, semantic
web technologies and visual analytics methodologies. This approach
symbiotically combines system-side computation, data storage and semantic web
services capabilities with human-side perceptive skills, cognitive reasoning
and domain knowledge. We introduce a novel \textit{continuous refinement model}
to gradually minimize the real-world noise and dissolve heterogeneity in data
and metadata such that the information gain can be maximized. Our contribution
lies in
\begin{inparaenum}[(1)]
  \item an \textit{integrated analytical pipeline} which includes
  \item \textit{smart semantic web services}, 
  \item \textit{domain expert knowledge extraction and generation }as
well as
  \item \textit{modelling uncertainty} to process
  high volume, high velocity and high dimensional spatio-temporal data from
  unreliable and heterogeneous sources.
\end{inparaenum}
In Section \ref{sec:related}, we discuss related work. The platform's design is
introduced in Section \ref{sec:biggis} through the continuous refinement model,
while major challenges are presented. Use cases are shown in Section
\ref{sec:use}. Finally, Section \ref{sec:concl} concludes and addresses future
work.

\section{Related Work}
\label{sec:related}
Challenges related to the nature of big data has
lead to the evolution of new big data management and analytics architectures
embracing big data-aware GIS~\cite{Peng2014}. Marz proposes the
\textit{lambda architecture}~\cite{Marz2013}, a generic, scalable and
fault-tolerant data processing system design. By decomposing the problem into
three layers, namely batch layer, speed layer, and serving layer, this
architecture hybridly combines batch analytics on historic data and stream
analytics on streaming data to overcome eachs single weakenesses. Thakur et al.
introduce \textit{PlanetSense}~\cite{Thakur2015}, a real-time streaming and
spatio-temporal analytics platform for gathering geo-spatial intelligence from
open source data. Based on the lambda architecture, this platform enriches
large volumes of historic data by harvesting real-time data on the fly, e.g.,
social media, or passive and participatory sensors. While this design allows
for adhoc analysis ability during batch runs, the processing logic has to be
implemented twice. In a more recent approach, Kreps criticizes the
overall complexity of the lambda architecture and presents the \textit{kappa
architecture}~\cite{Kreps2014}, which simplifies the systems' design
by neglecting the batch layer. To replace batch processing, static data is
quickly fed through the streaming engine. A representative is
\textit{Plasmap}\footnote{\url{https://plasmap.io/}}, a high performance
geo-processing platform that provides a lightweight, interactive query language
for high-performance location discovery based on OpenStreetMap. In contrast to
BigGIS, both PlanetSense and Plasmap do not apply reasoning on semantic
metadata and domain expert knowledge during runtime. However, developments in
the field of \textit{semantic web technologies} show the opportunity of adding 
higher semantic  levels  to existing frameworks in order to improve their 
usage and ease scalability, allowing for reasoning and comprehensive
responses~\cite{Tanasescu2006, Frank.2016a, Frank.2016b}. Analyses are often
performed in a descriptive, predictive or prescriptive way. While the
descriptive analysis visualizes the status quo, predictive and prescriptive
analysis focuses on future-oriented planning. As a result, the underlying model
and the visualization have to be tightly coupled in order for users to gain
knowledge. Users have the possibility to interactively alter a model's
parameters according to their knowledge, consequently the visualization adjusts
to the model in a feedback-loop. Knowledge generation is one important research
area where \textit{visual analytics} is of great use~\cite{Keim2008, Keim2010},
especially when considering uncertainty of heterogeneous spatio-temporal data
from various data sources \cite{SSK+16a}. J\"ackle et al. present one possible
visualization technique \cite{JSBK15} for data and uncertainties of large
spatial datasets, which is crucial within use cases where both facets are of
importance for decision-making.
%Andrienko et al. states that geovisual analytics need new approaches to
%deal with the complexity of data and address a research agenda for
%working with spatio-temporal data \cite{Andrienko2010}.
% Plasmap differs from BigGIS in that domain expert knowledge is not
%used to train the system.

\section{B\MakeLowercase{ig}GIS Platform}
\label{sec:biggis}

\subsection{Continuous Refinement Model in BigGIS}
\label{sec:crm}
In this section, we briefly describe the continuous refinement model in
BigGIS, which extends the knowledge generation model for visual
analytics \cite{SSS+14a}. This will on one hand allow to steadily
improve the analysis results, e.g., by updating deployed machine learning
models, and on the other hand to build the user's trust in these results by
creating awareness of underlying uncertainties and data provenance which is key
for providing meaningful predictive and prescriptive decision support in
various fields \cite{SSK+16a}. We consider uncertainty to be reciprocally
related to generating new insights and consequently knowledge. Thus, modelling
uncertainty is a crucial task in BigGIS. From a high-level perspective, our
approach consists of an integrated analytics pipeline which blends big data
analytics and semantic web services on system-side with domain expert
knowledge on human-side, thereby modelling uncertainty to continuously refine
results to generate new knowledge as shown in Figure \ref{fig:biggisworkflow}.

\begin{figure}
\centering
	\includegraphics[width=\linewidth]{figures/biggis-workflow_v6}
	\caption{Continuous refinement model in BigGIS.}
	\label{fig:biggisworkflow}
\end{figure}

\subsubsection{Integrated Analytics Pipeline}
The analytics pipeline is the core of the continuous refinement model. A key
abstraction within this model are specific access points called
\textit{refinement gates} (see yellow squares in Figure
\ref{fig:biggisworkflow}). Refinement gates allow for smart semantic web
services, external domain expert knowledge and user interaction to enter the
pipeline at arbitrary stages during analyses to continuously improve
data management and analyses results, e.g., to support data preparation, to
automatically deploy data transformation workflows, to provide domain expert
knowledge in order to train machine learning models for pattern detection or to
manipulate visualizations. 

\subsubsection{Smart Semantic Web Services}
Locating all available data sources that are relevant for meaningful findings
in analytical processes is hard to do when it has to be done manually. Semantic
web technologies help to describe data sources using standard vocabularies.
Furthermore, reasoning on the logical entailments helps in discovering suitable
sources even if they are described differently, providing a two-level support
for users through what we call \textit{Linked APIs} and \textit{Cognitive
Apps}. The former abstracts away the user from manually composing data
integration workflows to unify heterogeneous data sources by using an
appropriate ontology that supports the system (direct semantic support). The
latter is a flexible service that is aware of a situational context and capable
of sharing it with other services (indirect semantic support).

\subsubsection{Domain Expert Knowledge Extraction and Generation}
The user is another relevant part in the continuous refinement model who is
either provided with additional domain expert knowledge by another person or
she herself is the expert in a specific field of application (direct expert
knowledge). Overall, we see the continuous refinement process as a knowledge
transfer from human to system which is reinforced by smart semantic web
services that reason on semantically described data. Thereby, human knowledge
is introduced to the system that can contain additional domain specific
information and constraints. By doing so, big data analytics can
\begin{inparaenum}[(1)]
 	\item leverage perceptive skills, cognitive reasoning of human analysis
to be able to establish hidden connections between data and the problem domain
and
	\item continuously refine the analyses quality and results.
\end{inparaenum}
The system intelligently learns from the provided external domain knowledge,
such that it can reuse it for future tasks (indirect expert support). Thus,
leading to an increasing likelihood of relevant findings by a user during the
course of exploration and eventually to generating new knowledge.


\subsubsection{Modelling Uncertainty}
Uncertainty is inherent in data as well as in
models~\cite{cressie2015statistics}. While this is often obvious in data such
as volunteered geographic information (VGI) and participatory sensing data,
this holds true for all available data. Models derived from data
are only perfectly applicable for the data upon which they are learned.
Thereby, domain expert knowledge has some inherent uncertainty as well. To
handle these uncertainties, we express them as \textit{conditional
probabilities}. These conditional probabilities allow us to evaluate and model
the uncertainty of each data point as well as forecast an analytical model. We
apply semantic reasoning on the provenance information of data sources in order
to infer a level of uncertainty that can be considered in the analytical
processes. We use \textit{bayesian hierarchical
models}~\cite{cressie2015statistics} to be able to cope with the conditional
probabilities quantified by the semantic reasoning. The idea behind this is
that we can model different parameters by their joint probability distribution.
Each parameter can be modelled by hyperparameters, which are again probability
distributions. The resulting models are probability distributions as well,
which can be used in our continuous refinement model. By doing so, we can
model, examine and present the uncertainty at each stage of the process to
enable the user of BigGIS to make a well-informed decision.

\subsection{Challenges}
\label{sec:chls}
We identify three major challenges.

\subsubsection{Varying big data related requirements}
Data volume and velocity are well-managed requirements through highly
scalable cloud-based big data architectures such as lambda architecture or
kappa architecture. Still, the field of application specifies the degrees of
big data related requirements. Thus, to provide a unified platform, efficiently
managing the complex backend ecosystem for varying requirements is a
non-trivial task. We approach this challenge by leveraging Apache
Mesos\footnote{\url{http://mesos.apache.org/}} in combination with
container technologies such as Docker \footnote{\url{https://www.docker.com/}}.
In addition, dealing with data and schema heterogeneity and
inherent uncertainty is another relevant field of research that BigGIS
addresses. Preconditions for meaningful findings in GIS are accurate,
consistent and complete data as input for analytical processes. However,
sources of spatio-temporal data are distributed and quality of the data is
varying, especially when considering uncertain data such as VGI. We intend to
address this challenge by a smart data integration approach which is based on
semantically described data sources and data transformation services as
discussed in~\cite{Frank.2016a}. Smart web services dynamically compose
workflows of data sources and data transformation services adopted to the
requirements of different GISs based on the semantic
metadata~\cite{Frank.2016b}.

\subsubsection{Dimensionality reduction}
The continuous refinement model employed in BigGIS is
dependent on a fast computation of possible data sources. The used data are
high-dimensional and models built upon this data can easily fall to the curse
of dimensionality. Also, while the presented architecture can handle the
challenges of big data, it is not always possible to transfer all the raw data
to our pipeline. In case of an automated sensor, e.g., on an unmanned aerial
vehicle, the transfer rate is dependent on the available bandwidth.
BigGIS aims to deal with the challenge of dimensionality reduction for
spatio-temporal data, balancing between the robustness of a model and the
adaptability to training and input data. 

\subsubsection{Bias-variance trade-off}
The bias-variance trade-off~\cite{Hastie2009} is of particular
interest in BigGIS, as the modelling of uncertainty in the continuous
refinement model is inherently connected to this. Generally, solving this
trade-off optimally is highly depending on the specific use case. Providing the
user with sufficient information to reason and generate knowledge under these
restrictions is one demanding problem. Here, the challenge lies in the speed of
computation, the different level of expertise for each user and the available
bandwidth to transfer information back to the user and the analytics pipeline.

\section{Use Cases}
\label{sec:use}
%\textcolor{red}{ToDO@all: please review and comment}
BigGIS will support decision-making in multiple use cases that require
processing of large and heterogeneous spatio-temporal data from unreliable
sources. The prototype will be evaluated on three use cases: 
\begin{inparaenum}[(1)]
	\item smart city and health, i.e., heat stress in urban areas,
	\item environmental management, i.e., spread of invasive species,
	\item emergency management, i.e., identification and dispersion of hazardous
gas in chemical accidents.
\end{inparaenum}
These scenarios represent diverse categories of application domains that each 
address varying big data related requirements. 
%In brief, one example is
%the environmental management scenario, where farmers can provide real-time
%data
%(velocity) from differing sources (variety), e.g. private weather stations,
%photos of invasive 
%species, or text messages about contaminated areas, though arriving with high 
%uncertainty (veracity). The experts domain knowledge helps to train
%classifiers
%in BigGIS on already available labeled datasets (volume) that, in addition to 
%further semantically described sources, helps conducting spatio-temporal 
%statistics such as hot spot analyses to make better predictions on potentially
%jeopardized areas. Not only are farmers informed about the condition of their 
%fields (descriptive) but also about risk potential of contamination 
%(predictive), which lastly results in suggestions to perform certain 
%counteractive measures (prescriptive).
%%this is just an alternative scenario; speech has to be updated.
In brief, an illustrating scenario in the aforementioned emergency management
use case is supporting rescue forces in assessing and managing large-scale and
complex chemical disasters. Providing an in-depth overview of the current
situation within a small time frame (velocity) is crucial to prevent exposing
the surrounding population to any hazardous substances. Recent developments in
the field of mobile robotics allow using in-situ components such as
autonomously flying unmanned aerial vehicles equipped with hyper spectral
cameras to scan the affected area for hazardous gases producing several
gigabytes of raw data per mission (volume). In addition, differing sources
(variety), e.g., weather stations, VGI or participatory sensing data, can be
integrated in BigGIS, though arriving with high uncertainty (veracity). The
combination of those datasets with various other semantically described data
sources, helps conducting spatio-temporal statistics. Furthermore, the experts
domain knowledge is used to train classifiers in order to automatically
classify the hazardous content and identify contaminated areas. Conditional
probabilities are computed to forecast the dispersion of the hazardous smoke
and visualized in risk maps to highlight potentially endangered areas. Not only
are the rescue forces informed about the current situation (descriptive), but
also about the risk potential of surrounding areas (predictive), which can be
used to automatically alert further public authorities and organizations that
would be enabled to perform specifically targeted measures (prescriptive).

\section{Conclusions and Future Work}
\label{sec:concl}
Big geo data will continually grow during the next years. The
rapidly increasing distribution and importance of remote sensing data, e.g.,
from unmanned aerial vehicles, and participatory sensing data as well as the
emergence of new data sources lead to more diverse, larger and unreliable data.
In this paper, we proposed BigGIS, a next generation predictive and
prescriptive GIS, that leverages big data analytics, semantic web technologies
and visual analytics methodologies through a novel continuous refinement model.
We showed the key architectural elements to master heterogeneity and
uncertainty in spatio-temporal big data to generate meaningful knowledge and
identified three main challenges. Currently, we are working on an integrated
prototype to support each of the presented use cases.

%\end{document}  % This is where a 'short' article might terminate

%ACKNOWLEDGMENTS are optional
\section{Acknowledgements}
\label{sec:ack}
%This work has been developed in the project BigGIS. BigGIS (reference
%number: 01IS14012) is funded by the German ministry of education and research
%(BMBF) within the research programme ICT 2020.
The project BigGIS (reference number: 01IS14012) is funded by the Federal
Ministry of Education and Research (BMBF) within the frame of the research
programme ``Management and Analysis of Big Data'' in ``ICT 2020 --
Research for Innovations''.

%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{biggis-paper}  % sigproc.bib is the name of the
%Bibliography
%this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%
%APPENDICES are optional
%\balancecolumns
%\appendix
%Appendix A


\end{document}
